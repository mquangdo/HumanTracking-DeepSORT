{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11159873,"sourceType":"datasetVersion","datasetId":6963438}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install roboflow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T10:26:29.253438Z","iopub.execute_input":"2025-03-25T10:26:29.253758Z","iopub.status.idle":"2025-03-25T10:26:32.650555Z","shell.execute_reply.started":"2025-03-25T10:26:29.253730Z","shell.execute_reply":"2025-03-25T10:26:32.649740Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: roboflow in /usr/local/lib/python3.10/dist-packages (1.1.58)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from roboflow) (2025.1.31)\nRequirement already satisfied: idna==3.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7)\nRequirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.7)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.5)\nRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.4)\nRequirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.10.0.84)\nRequirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (11.0.0)\nRequirement already satisfied: pillow-heif>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.22.0)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.9.0.post0)\nRequirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.1.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.32.3)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.17.0)\nRequirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.3.0)\nRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.67.1)\nRequirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.2)\nRequirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\nRequirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->roboflow) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->roboflow) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->roboflow) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->roboflow) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->roboflow) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->roboflow) (2.4.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.3.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.55.3)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.18.5->roboflow) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.18.5->roboflow) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.18.5->roboflow) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.18.5->roboflow) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.18.5->roboflow) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# from roboflow import Roboflow\n# import os\n\n# rf = Roboflow(api_key=\"E6klOjeOgdAuTJyFXCpD\")\n# project = rf.workspace(\"objectdetection\").project('reid-b9lox')\n\n# dataset_path = \"/kaggle/input/reid-dataset/dataset_ETHZ/seq1\"\n\n# for folder_name in os.listdir(dataset_path):\n#     folder_path = os.path.join(dataset_path, folder_name)\n#     if os.path.isdir(folder_path):\n#         for image_file in os.listdir(folder_path):\n#             if image_file.endswith((\".jpg\", \".png\")): \n#                 image_path = os.path.join(folder_path, image_file)\n#                 project.single_upload(\n#                     image_path=image_path,\n#                     tag_names=[folder_name]  # Dùng tên folder làm class\n#                 )\n#                 print(f\"Uploaded {image_file} from {folder_name}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:22:28.366267Z","iopub.execute_input":"2025-03-25T12:22:28.366582Z","iopub.status.idle":"2025-03-25T12:22:28.370702Z","shell.execute_reply.started":"2025-03-25T12:22:28.366554Z","shell.execute_reply":"2025-03-25T12:22:28.369865Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\n# Đường dẫn đến dữ liệu từ Roboflow\ndata_dir = \"/kaggle/input/reid-dataset/dataset_ETHZ/seq1\"  # Thay bằng thư mục sau khi giải nén (chứa train/valid/test)\n\n# Transform dữ liệu cho MobileNetV2\ntransformation = transforms.Compose([\n    transforms.Resize((250, 250)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225] )\n    ])\n\ntrain_data = torchvision.datasets.ImageFolder(root=data_dir, transform=transformation)\ntrain_data_loader = DataLoader(train_data, batch_size=32, shuffle=True,  num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:27:54.027610Z","iopub.execute_input":"2025-03-25T12:27:54.028055Z","iopub.status.idle":"2025-03-25T12:27:56.421778Z","shell.execute_reply.started":"2025-03-25T12:27:54.028012Z","shell.execute_reply":"2025-03-25T12:27:56.420586Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Tải MobileNetV2 với trọng số pre-trained từ ImageNet\nmodel = models.mobilenet_v2(pretrained=True)\n\n# Đóng băng các layer của mô hình (chỉ huấn luyện lớp fully connected)\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Thay lớp fully connected để phù hợp với 83 class\nmodel.classifier[1] = nn.Sequential(nn.Linear(model.last_channel, 128), nn.Linear(128, 83))\n\n\n# Chuyển model sang GPU nếu có\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:27:58.501867Z","iopub.execute_input":"2025-03-25T12:27:58.502154Z","iopub.status.idle":"2025-03-25T12:27:58.632855Z","shell.execute_reply.started":"2025-03-25T12:27:58.502130Z","shell.execute_reply":"2025-03-25T12:27:58.632165Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def train(model, device, train_loader, optimizer, epoch):\n    # Set the model to training mode\n    model.train()\n    train_loss = 0\n    print(\"Epoch:\", epoch)\n    # Process the images in batches\n    for batch_idx, (data, target) in enumerate(train_data_loader):\n        # Use the CPU or GPU as appropriate\n        data, target = data.to(device), target.to(device)\n        \n        # Reset the optimizer\n        optimizer.zero_grad()\n        \n        # Push the data forward through the model layers\n        output = model(data)\n        \n        # Get the loss\n        loss = loss_criteria(output, target)\n        \n        # Keep a running total\n        train_loss += loss.item()\n        \n        # Backpropagate\n        loss.backward()\n        optimizer.step()\n        \n        # Print metrics for every 10 batches so we see some progress\n        if batch_idx % 10 == 0:\n            print('Training set [{}/{} ({:.0f}%)] Loss: {:.6f}'.format(\n                batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n            \n    # return average loss for the epoch\n    avg_loss = train_loss / (batch_idx+1)\n    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n    return avg_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:28:00.086298Z","iopub.execute_input":"2025-03-25T12:28:00.086581Z","iopub.status.idle":"2025-03-25T12:28:00.092462Z","shell.execute_reply.started":"2025-03-25T12:28:00.086560Z","shell.execute_reply":"2025-03-25T12:28:00.091520Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def test(model, device, test_loader):\n    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        batch_count = 0\n        for data, target in test_loader:\n            batch_count += 1\n            data, target = data.to(device), target.to(device)\n            \n            # Get the predicted classes for this batch\n            output = model(data)\n            \n            # Calculate the loss for this batch\n            test_loss += loss_criteria(output, target).item()\n            \n            # Calculate the accuracy for this batch\n            _, predicted = torch.max(output.data, 1)\n            correct += torch.sum(target==predicted).item()\n\n    # Calculate the average loss and total accuracy for this epoch\n    avg_loss = test_loss/batch_count\n    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        avg_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n    \n    # return average loss for the epoch\n    return avg_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:28:01.877096Z","iopub.execute_input":"2025-03-25T12:28:01.877390Z","iopub.status.idle":"2025-03-25T12:28:01.882984Z","shell.execute_reply.started":"2025-03-25T12:28:01.877366Z","shell.execute_reply":"2025-03-25T12:28:01.882060Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Use an \"Adam\" optimizer to adjust weights\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Specify the loss criteria\nloss_criteria = nn.CrossEntropyLoss()\n\n# Track metrics in these arrays\nepoch_nums = []\ntraining_loss = []\n\n# Train over 20 epochs (in a real scenario, you'd likely use many more)\nepochs = 20\nfor epoch in range(1, epochs + 1):\n        train_loss = train(model, device, train_data_loader, optimizer, epoch)\n        epoch_nums.append(epoch)\n        training_loss.append(train_loss)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:28:03.193623Z","iopub.execute_input":"2025-03-25T12:28:03.193972Z","iopub.status.idle":"2025-03-25T12:31:08.497613Z","shell.execute_reply.started":"2025-03-25T12:28:03.193946Z","shell.execute_reply":"2025-03-25T12:31:08.496712Z"}},"outputs":[{"name":"stdout","text":"Epoch: 1\nTraining set [0/4857 (0%)] Loss: 4.459498\nTraining set [320/4857 (7%)] Loss: 3.963038\nTraining set [640/4857 (13%)] Loss: 3.425424\nTraining set [960/4857 (20%)] Loss: 3.615683\nTraining set [1280/4857 (26%)] Loss: 3.627561\nTraining set [1600/4857 (33%)] Loss: 2.865065\nTraining set [1920/4857 (39%)] Loss: 2.784567\nTraining set [2240/4857 (46%)] Loss: 2.304363\nTraining set [2560/4857 (53%)] Loss: 2.105062\nTraining set [2880/4857 (59%)] Loss: 2.092680\nTraining set [3200/4857 (66%)] Loss: 2.222371\nTraining set [3520/4857 (72%)] Loss: 1.780229\nTraining set [3840/4857 (79%)] Loss: 1.680151\nTraining set [4160/4857 (86%)] Loss: 1.788515\nTraining set [4480/4857 (92%)] Loss: 0.958950\nTraining set [4800/4857 (99%)] Loss: 1.201260\nTraining set: Average loss: 2.546337\nEpoch: 2\nTraining set [0/4857 (0%)] Loss: 1.277029\nTraining set [320/4857 (7%)] Loss: 0.774886\nTraining set [640/4857 (13%)] Loss: 1.028592\nTraining set [960/4857 (20%)] Loss: 1.338444\nTraining set [1280/4857 (26%)] Loss: 0.864562\nTraining set [1600/4857 (33%)] Loss: 0.716489\nTraining set [1920/4857 (39%)] Loss: 0.509988\nTraining set [2240/4857 (46%)] Loss: 0.927069\nTraining set [2560/4857 (53%)] Loss: 0.727652\nTraining set [2880/4857 (59%)] Loss: 0.566986\nTraining set [3200/4857 (66%)] Loss: 0.525195\nTraining set [3520/4857 (72%)] Loss: 1.039063\nTraining set [3840/4857 (79%)] Loss: 0.596226\nTraining set [4160/4857 (86%)] Loss: 0.606395\nTraining set [4480/4857 (92%)] Loss: 0.548888\nTraining set [4800/4857 (99%)] Loss: 0.314253\nTraining set: Average loss: 0.741462\nEpoch: 3\nTraining set [0/4857 (0%)] Loss: 0.495867\nTraining set [320/4857 (7%)] Loss: 0.672794\nTraining set [640/4857 (13%)] Loss: 0.223871\nTraining set [960/4857 (20%)] Loss: 0.528826\nTraining set [1280/4857 (26%)] Loss: 0.330986\nTraining set [1600/4857 (33%)] Loss: 0.336391\nTraining set [1920/4857 (39%)] Loss: 0.353563\nTraining set [2240/4857 (46%)] Loss: 0.442491\nTraining set [2560/4857 (53%)] Loss: 0.141635\nTraining set [2880/4857 (59%)] Loss: 0.589923\nTraining set [3200/4857 (66%)] Loss: 0.247517\nTraining set [3520/4857 (72%)] Loss: 0.327426\nTraining set [3840/4857 (79%)] Loss: 0.362412\nTraining set [4160/4857 (86%)] Loss: 0.394622\nTraining set [4480/4857 (92%)] Loss: 0.490462\nTraining set [4800/4857 (99%)] Loss: 0.189509\nTraining set: Average loss: 0.345710\nEpoch: 4\nTraining set [0/4857 (0%)] Loss: 0.272085\nTraining set [320/4857 (7%)] Loss: 0.265408\nTraining set [640/4857 (13%)] Loss: 0.150580\nTraining set [960/4857 (20%)] Loss: 0.192304\nTraining set [1280/4857 (26%)] Loss: 0.237754\nTraining set [1600/4857 (33%)] Loss: 0.297005\nTraining set [1920/4857 (39%)] Loss: 0.306747\nTraining set [2240/4857 (46%)] Loss: 0.204371\nTraining set [2560/4857 (53%)] Loss: 0.158431\nTraining set [2880/4857 (59%)] Loss: 0.099799\nTraining set [3200/4857 (66%)] Loss: 0.272045\nTraining set [3520/4857 (72%)] Loss: 0.144151\nTraining set [3840/4857 (79%)] Loss: 0.164493\nTraining set [4160/4857 (86%)] Loss: 0.157894\nTraining set [4480/4857 (92%)] Loss: 0.180225\nTraining set [4800/4857 (99%)] Loss: 0.076778\nTraining set: Average loss: 0.224220\nEpoch: 5\nTraining set [0/4857 (0%)] Loss: 0.162343\nTraining set [320/4857 (7%)] Loss: 0.278002\nTraining set [640/4857 (13%)] Loss: 0.131740\nTraining set [960/4857 (20%)] Loss: 0.227695\nTraining set [1280/4857 (26%)] Loss: 0.124193\nTraining set [1600/4857 (33%)] Loss: 0.059070\nTraining set [1920/4857 (39%)] Loss: 0.115139\nTraining set [2240/4857 (46%)] Loss: 0.132564\nTraining set [2560/4857 (53%)] Loss: 0.466084\nTraining set [2880/4857 (59%)] Loss: 0.166650\nTraining set [3200/4857 (66%)] Loss: 0.267896\nTraining set [3520/4857 (72%)] Loss: 0.192112\nTraining set [3840/4857 (79%)] Loss: 0.059134\nTraining set [4160/4857 (86%)] Loss: 0.264487\nTraining set [4480/4857 (92%)] Loss: 0.133281\nTraining set [4800/4857 (99%)] Loss: 0.144331\nTraining set: Average loss: 0.161371\nEpoch: 6\nTraining set [0/4857 (0%)] Loss: 0.053317\nTraining set [320/4857 (7%)] Loss: 0.100262\nTraining set [640/4857 (13%)] Loss: 0.087033\nTraining set [960/4857 (20%)] Loss: 0.309532\nTraining set [1280/4857 (26%)] Loss: 0.113585\nTraining set [1600/4857 (33%)] Loss: 0.105749\nTraining set [1920/4857 (39%)] Loss: 0.225135\nTraining set [2240/4857 (46%)] Loss: 0.083386\nTraining set [2560/4857 (53%)] Loss: 0.204701\nTraining set [2880/4857 (59%)] Loss: 0.051511\nTraining set [3200/4857 (66%)] Loss: 0.146125\nTraining set [3520/4857 (72%)] Loss: 0.127701\nTraining set [3840/4857 (79%)] Loss: 0.111232\nTraining set [4160/4857 (86%)] Loss: 0.087390\nTraining set [4480/4857 (92%)] Loss: 0.076271\nTraining set [4800/4857 (99%)] Loss: 0.117375\nTraining set: Average loss: 0.116832\nEpoch: 7\nTraining set [0/4857 (0%)] Loss: 0.158331\nTraining set [320/4857 (7%)] Loss: 0.084485\nTraining set [640/4857 (13%)] Loss: 0.066490\nTraining set [960/4857 (20%)] Loss: 0.132887\nTraining set [1280/4857 (26%)] Loss: 0.184778\nTraining set [1600/4857 (33%)] Loss: 0.087651\nTraining set [1920/4857 (39%)] Loss: 0.211273\nTraining set [2240/4857 (46%)] Loss: 0.124228\nTraining set [2560/4857 (53%)] Loss: 0.135229\nTraining set [2880/4857 (59%)] Loss: 0.032516\nTraining set [3200/4857 (66%)] Loss: 0.021466\nTraining set [3520/4857 (72%)] Loss: 0.049254\nTraining set [3840/4857 (79%)] Loss: 0.126250\nTraining set [4160/4857 (86%)] Loss: 0.178841\nTraining set [4480/4857 (92%)] Loss: 0.227913\nTraining set [4800/4857 (99%)] Loss: 0.089401\nTraining set: Average loss: 0.098308\nEpoch: 8\nTraining set [0/4857 (0%)] Loss: 0.052804\nTraining set [320/4857 (7%)] Loss: 0.040711\nTraining set [640/4857 (13%)] Loss: 0.185454\nTraining set [960/4857 (20%)] Loss: 0.058774\nTraining set [1280/4857 (26%)] Loss: 0.042423\nTraining set [1600/4857 (33%)] Loss: 0.128930\nTraining set [1920/4857 (39%)] Loss: 0.069210\nTraining set [2240/4857 (46%)] Loss: 0.054063\nTraining set [2560/4857 (53%)] Loss: 0.074715\nTraining set [2880/4857 (59%)] Loss: 0.104385\nTraining set [3200/4857 (66%)] Loss: 0.059751\nTraining set [3520/4857 (72%)] Loss: 0.056164\nTraining set [3840/4857 (79%)] Loss: 0.017377\nTraining set [4160/4857 (86%)] Loss: 0.227448\nTraining set [4480/4857 (92%)] Loss: 0.017789\nTraining set [4800/4857 (99%)] Loss: 0.055333\nTraining set: Average loss: 0.077196\nEpoch: 9\nTraining set [0/4857 (0%)] Loss: 0.047814\nTraining set [320/4857 (7%)] Loss: 0.023579\nTraining set [640/4857 (13%)] Loss: 0.014680\nTraining set [960/4857 (20%)] Loss: 0.115700\nTraining set [1280/4857 (26%)] Loss: 0.327610\nTraining set [1600/4857 (33%)] Loss: 0.026279\nTraining set [1920/4857 (39%)] Loss: 0.027425\nTraining set [2240/4857 (46%)] Loss: 0.050861\nTraining set [2560/4857 (53%)] Loss: 0.046000\nTraining set [2880/4857 (59%)] Loss: 0.040610\nTraining set [3200/4857 (66%)] Loss: 0.102850\nTraining set [3520/4857 (72%)] Loss: 0.034283\nTraining set [3840/4857 (79%)] Loss: 0.035091\nTraining set [4160/4857 (86%)] Loss: 0.043045\nTraining set [4480/4857 (92%)] Loss: 0.119603\nTraining set [4800/4857 (99%)] Loss: 0.045473\nTraining set: Average loss: 0.072721\nEpoch: 10\nTraining set [0/4857 (0%)] Loss: 0.113449\nTraining set [320/4857 (7%)] Loss: 0.042581\nTraining set [640/4857 (13%)] Loss: 0.089457\nTraining set [960/4857 (20%)] Loss: 0.010194\nTraining set [1280/4857 (26%)] Loss: 0.025751\nTraining set [1600/4857 (33%)] Loss: 0.070952\nTraining set [1920/4857 (39%)] Loss: 0.035049\nTraining set [2240/4857 (46%)] Loss: 0.089709\nTraining set [2560/4857 (53%)] Loss: 0.094448\nTraining set [2880/4857 (59%)] Loss: 0.106777\nTraining set [3200/4857 (66%)] Loss: 0.071293\nTraining set [3520/4857 (72%)] Loss: 0.134762\nTraining set [3840/4857 (79%)] Loss: 0.078889\nTraining set [4160/4857 (86%)] Loss: 0.092832\nTraining set [4480/4857 (92%)] Loss: 0.015197\nTraining set [4800/4857 (99%)] Loss: 0.013580\nTraining set: Average loss: 0.060790\nEpoch: 11\nTraining set [0/4857 (0%)] Loss: 0.018021\nTraining set [320/4857 (7%)] Loss: 0.035588\nTraining set [640/4857 (13%)] Loss: 0.039058\nTraining set [960/4857 (20%)] Loss: 0.059806\nTraining set [1280/4857 (26%)] Loss: 0.053342\nTraining set [1600/4857 (33%)] Loss: 0.018678\nTraining set [1920/4857 (39%)] Loss: 0.023789\nTraining set [2240/4857 (46%)] Loss: 0.016372\nTraining set [2560/4857 (53%)] Loss: 0.027039\nTraining set [2880/4857 (59%)] Loss: 0.060766\nTraining set [3200/4857 (66%)] Loss: 0.008729\nTraining set [3520/4857 (72%)] Loss: 0.062880\nTraining set [3840/4857 (79%)] Loss: 0.014811\nTraining set [4160/4857 (86%)] Loss: 0.031761\nTraining set [4480/4857 (92%)] Loss: 0.136377\nTraining set [4800/4857 (99%)] Loss: 0.052928\nTraining set: Average loss: 0.060235\nEpoch: 12\nTraining set [0/4857 (0%)] Loss: 0.039069\nTraining set [320/4857 (7%)] Loss: 0.045811\nTraining set [640/4857 (13%)] Loss: 0.127215\nTraining set [960/4857 (20%)] Loss: 0.010234\nTraining set [1280/4857 (26%)] Loss: 0.008592\nTraining set [1600/4857 (33%)] Loss: 0.006871\nTraining set [1920/4857 (39%)] Loss: 0.030840\nTraining set [2240/4857 (46%)] Loss: 0.176825\nTraining set [2560/4857 (53%)] Loss: 0.010958\nTraining set [2880/4857 (59%)] Loss: 0.029001\nTraining set [3200/4857 (66%)] Loss: 0.097402\nTraining set [3520/4857 (72%)] Loss: 0.029013\nTraining set [3840/4857 (79%)] Loss: 0.013061\nTraining set [4160/4857 (86%)] Loss: 0.123096\nTraining set [4480/4857 (92%)] Loss: 0.038891\nTraining set [4800/4857 (99%)] Loss: 0.026825\nTraining set: Average loss: 0.048208\nEpoch: 13\nTraining set [0/4857 (0%)] Loss: 0.081299\nTraining set [320/4857 (7%)] Loss: 0.055816\nTraining set [640/4857 (13%)] Loss: 0.042403\nTraining set [960/4857 (20%)] Loss: 0.021630\nTraining set [1280/4857 (26%)] Loss: 0.047141\nTraining set [1600/4857 (33%)] Loss: 0.046388\nTraining set [1920/4857 (39%)] Loss: 0.036089\nTraining set [2240/4857 (46%)] Loss: 0.018594\nTraining set [2560/4857 (53%)] Loss: 0.035963\nTraining set [2880/4857 (59%)] Loss: 0.042449\nTraining set [3200/4857 (66%)] Loss: 0.021340\nTraining set [3520/4857 (72%)] Loss: 0.022145\nTraining set [3840/4857 (79%)] Loss: 0.240024\nTraining set [4160/4857 (86%)] Loss: 0.011251\nTraining set [4480/4857 (92%)] Loss: 0.138280\nTraining set [4800/4857 (99%)] Loss: 0.284735\nTraining set: Average loss: 0.046169\nEpoch: 14\nTraining set [0/4857 (0%)] Loss: 0.174316\nTraining set [320/4857 (7%)] Loss: 0.043821\nTraining set [640/4857 (13%)] Loss: 0.004414\nTraining set [960/4857 (20%)] Loss: 0.021815\nTraining set [1280/4857 (26%)] Loss: 0.021658\nTraining set [1600/4857 (33%)] Loss: 0.133530\nTraining set [1920/4857 (39%)] Loss: 0.046964\nTraining set [2240/4857 (46%)] Loss: 0.047586\nTraining set [2560/4857 (53%)] Loss: 0.039166\nTraining set [2880/4857 (59%)] Loss: 0.017903\nTraining set [3200/4857 (66%)] Loss: 0.105975\nTraining set [3520/4857 (72%)] Loss: 0.033253\nTraining set [3840/4857 (79%)] Loss: 0.025082\nTraining set [4160/4857 (86%)] Loss: 0.020492\nTraining set [4480/4857 (92%)] Loss: 0.028518\nTraining set [4800/4857 (99%)] Loss: 0.031584\nTraining set: Average loss: 0.050821\nEpoch: 15\nTraining set [0/4857 (0%)] Loss: 0.042347\nTraining set [320/4857 (7%)] Loss: 0.052807\nTraining set [640/4857 (13%)] Loss: 0.162192\nTraining set [960/4857 (20%)] Loss: 0.076432\nTraining set [1280/4857 (26%)] Loss: 0.051862\nTraining set [1600/4857 (33%)] Loss: 0.055072\nTraining set [1920/4857 (39%)] Loss: 0.004832\nTraining set [2240/4857 (46%)] Loss: 0.011096\nTraining set [2560/4857 (53%)] Loss: 0.031690\nTraining set [2880/4857 (59%)] Loss: 0.016892\nTraining set [3200/4857 (66%)] Loss: 0.007492\nTraining set [3520/4857 (72%)] Loss: 0.081143\nTraining set [3840/4857 (79%)] Loss: 0.080121\nTraining set [4160/4857 (86%)] Loss: 0.096320\nTraining set [4480/4857 (92%)] Loss: 0.040854\nTraining set [4800/4857 (99%)] Loss: 0.009815\nTraining set: Average loss: 0.039214\nEpoch: 16\nTraining set [0/4857 (0%)] Loss: 0.026574\nTraining set [320/4857 (7%)] Loss: 0.023940\nTraining set [640/4857 (13%)] Loss: 0.035430\nTraining set [960/4857 (20%)] Loss: 0.014718\nTraining set [1280/4857 (26%)] Loss: 0.024956\nTraining set [1600/4857 (33%)] Loss: 0.101780\nTraining set [1920/4857 (39%)] Loss: 0.017001\nTraining set [2240/4857 (46%)] Loss: 0.201756\nTraining set [2560/4857 (53%)] Loss: 0.009083\nTraining set [2880/4857 (59%)] Loss: 0.008905\nTraining set [3200/4857 (66%)] Loss: 0.015707\nTraining set [3520/4857 (72%)] Loss: 0.026007\nTraining set [3840/4857 (79%)] Loss: 0.087915\nTraining set [4160/4857 (86%)] Loss: 0.020712\nTraining set [4480/4857 (92%)] Loss: 0.007484\nTraining set [4800/4857 (99%)] Loss: 0.011822\nTraining set: Average loss: 0.050571\nEpoch: 17\nTraining set [0/4857 (0%)] Loss: 0.019084\nTraining set [320/4857 (7%)] Loss: 0.006684\nTraining set [640/4857 (13%)] Loss: 0.233633\nTraining set [960/4857 (20%)] Loss: 0.020068\nTraining set [1280/4857 (26%)] Loss: 0.006422\nTraining set [1600/4857 (33%)] Loss: 0.011236\nTraining set [1920/4857 (39%)] Loss: 0.031238\nTraining set [2240/4857 (46%)] Loss: 0.017931\nTraining set [2560/4857 (53%)] Loss: 0.018159\nTraining set [2880/4857 (59%)] Loss: 0.074282\nTraining set [3200/4857 (66%)] Loss: 0.025305\nTraining set [3520/4857 (72%)] Loss: 0.090358\nTraining set [3840/4857 (79%)] Loss: 0.026635\nTraining set [4160/4857 (86%)] Loss: 0.022448\nTraining set [4480/4857 (92%)] Loss: 0.014993\nTraining set [4800/4857 (99%)] Loss: 0.043651\nTraining set: Average loss: 0.037140\nEpoch: 18\nTraining set [0/4857 (0%)] Loss: 0.006494\nTraining set [320/4857 (7%)] Loss: 0.063974\nTraining set [640/4857 (13%)] Loss: 0.073400\nTraining set [960/4857 (20%)] Loss: 0.088147\nTraining set [1280/4857 (26%)] Loss: 0.054028\nTraining set [1600/4857 (33%)] Loss: 0.009811\nTraining set [1920/4857 (39%)] Loss: 0.028317\nTraining set [2240/4857 (46%)] Loss: 0.056408\nTraining set [2560/4857 (53%)] Loss: 0.095496\nTraining set [2880/4857 (59%)] Loss: 0.071459\nTraining set [3200/4857 (66%)] Loss: 0.015445\nTraining set [3520/4857 (72%)] Loss: 0.026177\nTraining set [3840/4857 (79%)] Loss: 0.003670\nTraining set [4160/4857 (86%)] Loss: 0.036121\nTraining set [4480/4857 (92%)] Loss: 0.002794\nTraining set [4800/4857 (99%)] Loss: 0.005422\nTraining set: Average loss: 0.044823\nEpoch: 19\nTraining set [0/4857 (0%)] Loss: 0.096328\nTraining set [320/4857 (7%)] Loss: 0.052464\nTraining set [640/4857 (13%)] Loss: 0.025387\nTraining set [960/4857 (20%)] Loss: 0.011697\nTraining set [1280/4857 (26%)] Loss: 0.359480\nTraining set [1600/4857 (33%)] Loss: 0.122063\nTraining set [1920/4857 (39%)] Loss: 0.004071\nTraining set [2240/4857 (46%)] Loss: 0.018875\nTraining set [2560/4857 (53%)] Loss: 0.028123\nTraining set [2880/4857 (59%)] Loss: 0.088732\nTraining set [3200/4857 (66%)] Loss: 0.150197\nTraining set [3520/4857 (72%)] Loss: 0.102215\nTraining set [3840/4857 (79%)] Loss: 0.045947\nTraining set [4160/4857 (86%)] Loss: 0.069162\nTraining set [4480/4857 (92%)] Loss: 0.054536\nTraining set [4800/4857 (99%)] Loss: 0.009956\nTraining set: Average loss: 0.040468\nEpoch: 20\nTraining set [0/4857 (0%)] Loss: 0.004502\nTraining set [320/4857 (7%)] Loss: 0.005048\nTraining set [640/4857 (13%)] Loss: 0.018078\nTraining set [960/4857 (20%)] Loss: 0.004690\nTraining set [1280/4857 (26%)] Loss: 0.010005\nTraining set [1600/4857 (33%)] Loss: 0.021012\nTraining set [1920/4857 (39%)] Loss: 0.032054\nTraining set [2240/4857 (46%)] Loss: 0.008027\nTraining set [2560/4857 (53%)] Loss: 0.040889\nTraining set [2880/4857 (59%)] Loss: 0.003568\nTraining set [3200/4857 (66%)] Loss: 0.004201\nTraining set [3520/4857 (72%)] Loss: 0.071366\nTraining set [3840/4857 (79%)] Loss: 0.036694\nTraining set [4160/4857 (86%)] Loss: 0.002521\nTraining set [4480/4857 (92%)] Loss: 0.030938\nTraining set [4800/4857 (99%)] Loss: 0.045798\nTraining set: Average loss: 0.026366\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"plt.plot(epoch_nums, training_loss)\nplt.grid('True')\nplt.title('Learning curve')\nplt.xlabel('Epochs')\nplt.ylabel('Loss value')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:31:51.442574Z","iopub.execute_input":"2025-03-25T12:31:51.443022Z","iopub.status.idle":"2025-03-25T12:31:51.641832Z","shell.execute_reply.started":"2025-03-25T12:31:51.442991Z","shell.execute_reply":"2025-03-25T12:31:51.640928Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"Text(0, 0.5, 'Loss value')"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"model_path = '/kaggle/working/reid_weights.pt'\ntorch.save(model, model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:32:47.425572Z","iopub.execute_input":"2025-03-25T12:32:47.425918Z","iopub.status.idle":"2025-03-25T12:32:47.480403Z","shell.execute_reply.started":"2025-03-25T12:32:47.425887Z","shell.execute_reply":"2025-03-25T12:32:47.479508Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:32:59.157255Z","iopub.execute_input":"2025-03-25T12:32:59.157576Z","iopub.status.idle":"2025-03-25T12:32:59.165032Z","shell.execute_reply.started":"2025-03-25T12:32:59.157552Z","shell.execute_reply":"2025-03-25T12:32:59.164215Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"MobileNetV2(\n  (features): Sequential(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU6(inplace=True)\n    )\n    (1): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (2): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (3): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (4): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (8): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (9): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (10): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (11): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (12): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (13): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (14): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (15): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (16): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (17): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (18): Conv2dNormActivation(\n      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU6(inplace=True)\n    )\n  )\n  (classifier): Sequential(\n    (0): Dropout(p=0.2, inplace=False)\n    (1): Sequential(\n      (0): Linear(in_features=1280, out_features=128, bias=True)\n      (1): Linear(in_features=128, out_features=83, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"pretrained_model = torch.load('/kaggle/working/reid_weights.pt')\npretrained_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:36:13.166957Z","iopub.execute_input":"2025-03-25T12:36:13.167250Z","iopub.status.idle":"2025-03-25T12:36:13.231718Z","shell.execute_reply.started":"2025-03-25T12:36:13.167228Z","shell.execute_reply":"2025-03-25T12:36:13.230889Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-36-6899f5ea3402>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  pretrained_model = torch.load('/kaggle/working/reid_weights.pt')\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"MobileNetV2(\n  (features): Sequential(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU6(inplace=True)\n    )\n    (1): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (2): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (3): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (4): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (8): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (9): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (10): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (11): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (12): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (13): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (14): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (15): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (16): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (17): InvertedResidual(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU6(inplace=True)\n        )\n        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (18): Conv2dNormActivation(\n      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU6(inplace=True)\n    )\n  )\n  (classifier): Sequential(\n    (0): Dropout(p=0.2, inplace=False)\n    (1): Sequential(\n      (0): Linear(in_features=1280, out_features=128, bias=True)\n      (1): Linear(in_features=128, out_features=83, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}